import Image from 'next/image'

<Image src="/system-design/web-crawler.png" style={{backgroundColor: "white"}} width={1200} height={800} />

### URL Fetcher

- SQS supports retries with configurable exponential backoff out of the box
- After a certain number of failures, as determined by the ApproximateReceiveCount, the message is moved to a dead-letter queue (DLQ)
- Respect Robots.txt

### Too-Large File
- User HEAD request to get Content-Length before real fetching

### DNS Caching

- Cache DNS lookups in our crawlers to reduce the number of DNS requests.

### URL Metadata

- Store a link for parsers to download the fetched raw content
- Store a content hash for raw contents. Create a index for fast lookup. 
- Store a depth for the crawled URL. Avoid infinite loop

### Dynamic Content
- Use a headless browser like Puppeteer to render the page and extract the content.
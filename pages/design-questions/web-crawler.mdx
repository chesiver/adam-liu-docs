### Use Case

- Service crawls a list of urls. Generates reverse index of words to pages containing the search terms.
- User inputs a search term and sees a list of relevant pages with titles and snippets the crawler generated.

### URL Frontier / Queue

Based on the implementation in the Mercator web crawler.

- Front queues - This set of queues implement prioritization. Each queue has a priority label. 
The higher the priority, the sooner the URL popped from the queue.
- Back queues - This set of queues implement politeness. Each queue contains URLs from only one host.
- Min - Heap - There is one entry for each back queue. Each entry contains the server/host address and the earliest time 
when the request can be made.
- Caller threads - When we extract an URL from the back queue this thread group fetches the URLs and assigns URLs to the back queues.


### Bottleneck of DNS Resolution

- Caching the IP Addresses
- Fetching the IP Addresses asynchronously - Have separate worker threads that send requests to the DNS servers. 

### Storage

- Store webpage content in an object store
- Store Link-to-link relationshiop in a graph database

### Deduplication

- Content deduplication. MD5 / SHA1 hash algorithm to generate document fingerprint.
- URL deduplication. Bloom filter.

### Reference 

https://leetcode.com/discuss/interview-question/system-design/2950256/High-Level-Design-of-a-Web-Crawler
https://www.youtube.com/watch?v=OmyHt0JM7HM
https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/web_crawler/README.md
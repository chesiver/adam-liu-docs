import Image from 'next/image'

<Image src="/system-design/web-crawler.png" style={{backgroundColor: "white"}} width={1200} height={800} />

### URL Fetcher

- SQS supports retries with configurable exponential backoff out of the box
- After a certain number of failures, as determined by the ApproximateReceiveCount, the message is moved to a dead-letter queue (DLQ)
- Respect Robots.txt

### Too-Large File
- User HEAD request to get Content-Length before real fetching

### DNS Caching

- Cache DNS lookups in our crawlers to reduce the number of DNS requests.

### URL Metadata

- Store a link for parsers to download the fetched raw content
- Store a content hash for raw contents. Create a index for fast lookup. 
- Store a depth for the crawled URL. Avoid infinite loop

### Dynamic Content
- Use a headless browser like Puppeteer to render the page and extract the content.


{/* ### Use Case

- Service crawls a list of urls. Generates reverse index of words to pages containing the search terms.
- User inputs a search term and sees a list of relevant pages with titles and snippets the crawler generated.

### URL Frontier / Queue

Based on the implementation in the Mercator web crawler.

- Front queues - This set of queues implement prioritization. Each queue has a priority label. 
The higher the priority, the sooner the URL popped from the queue.
- Back queues - This set of queues implement politeness. Each queue contains URLs from only one host.
- Min - Heap - There is one entry for each back queue. Each entry contains the server/host address and the earliest time 
when the request can be made.
- Caller threads - When we extract an URL from the back queue this thread group fetches the URLs and assigns URLs to the back queues.

### Bottleneck of DNS Resolution

- Caching the IP Addresses
- Fetching the IP Addresses asynchronously - Have separate worker threads that send requests to the DNS servers. 

### Distributed URL frontier

- Partition nodes by hash range of URL
- Partition nodes by hash range of Hosts. If there's a separate domain-name resolution service.

### Storage

- Store webpage content in an object store
- Store Link-to-link relationshiop in a graph database

### Deduplication

- Content deduplication. MD5 / SHA1 hash algorithm to generate document fingerprint.
- URL deduplication. Bloom filter.

### Inverted Index Service

- Asynchronous jobs to generate inverted index.

### Reference 

1. https://leetcode.com/discuss/interview-question/system-design/2950256/High-Level-Design-of-a-Web-Crawler
2. https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/web_crawler/README.md
3. https://www.youtube.com/watch?v=OmyHt0JM7HM
4. https://www.youtube.com/watch?v=MdWvMX4J-Vc */}